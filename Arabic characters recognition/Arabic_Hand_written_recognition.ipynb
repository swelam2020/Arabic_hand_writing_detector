{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_SRzI8E0DunM"
   },
   "outputs": [],
   "source": [
    "# Import main libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Import libraries needed for reading image and processing it\n",
    "import csv\n",
    "from PIL import Image\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "Rp9uUzqjD4vt",
    "outputId": "b16a5ca6-7595-4642-c400-da1112ff7128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13440 training arabic letter images of 32x32 pixels.\n",
      "There are 3360 testing arabic letter images of 32x32 pixels.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  1014  \\\n",
       "0     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "4     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "   1015  1016  1017  1018  1019  1020  1021  1022  1023  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 1024 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training letters images and labels files\n",
    "letters_training_images_file_path = \"csvTrainImages 13440x1024.csv\"\n",
    "letters_training_labels_file_path = \"csvTrainLabel 13440x1.csv\"\n",
    "# Testing letters images and labels files\n",
    "letters_testing_images_file_path = \"csvTestImages 3360x1024.csv\"\n",
    "letters_testing_labels_file_path = \"csvTestLabel 3360x1.csv\"\n",
    "\n",
    "# Loading dataset into dataframes\n",
    "training_letters_images = pd.read_csv(letters_training_images_file_path, header=None)\n",
    "training_letters_labels = pd.read_csv(letters_training_labels_file_path, header=None)\n",
    "testing_letters_images = pd.read_csv(letters_testing_images_file_path, header=None)\n",
    "testing_letters_labels = pd.read_csv(letters_testing_labels_file_path, header=None)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print(\"There are %d training arabic letter images of 32x32 pixels.\" %training_letters_images.shape[0])\n",
    "print(\"There are %d testing arabic letter images of 32x32 pixels.\" %testing_letters_images.shape[0])\n",
    "training_letters_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsEP5cCaEngx"
   },
   "outputs": [],
   "source": [
    "def convert_values_to_image(image_values, display=False):\n",
    "    image_array = np.asarray(image_values)\n",
    "    image_array = image_array.reshape(32,32).astype('uint8')\n",
    "    # The original dataset is reflected so we will flip it then rotate for a better view only.\n",
    "    image_array = np.flip(image_array, 0)\n",
    "    image_array = rotate(image_array, -90)\n",
    "    new_image = Image.fromarray(image_array)\n",
    "    if display == True:\n",
    "        new_image.show()\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "-Pnf_Q7nFIwl",
    "outputId": "2160d050-7810-41c3-d428-36e67fc8f4f7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAA10lEQVR4nGNgoDZgSl2qjE9eZ/f/T9Z45N3e/JmoiEee9c7/XGad/DDcKub+v3b1//8bgrjkmRv/////aYEuLnnjM/9/vr1hitN8xcf/b7rofCnEqYBnwhwFBp4HL6Rwu5GBgYGh9n8tfgUyP69y4lXAeuB/IDKfCV3B7x0MgehiqEDu83Mx/CpWotiBYQUDw04GcfwmKPw/jl8B/813Cnit+Lhf0Aq/ETb/V+I1geE5gxp+E/iufVTFa8Kn63z8eBWwiDKw4lXw5yHDf/yO0M1hw6+AzgAAogc/5Xjxr2wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32 at 0x7FAC3E1DBDD0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_values_to_image(training_letters_images.loc[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuzN5ts0Fax7"
   },
   "source": [
    "**Data Pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgxcrH3ZFgke"
   },
   "outputs": [],
   "source": [
    "#Normalization \n",
    "\n",
    "training_letters_images_scaled = training_letters_images.values.astype('float32')/255\n",
    "training_letters_labels = training_letters_labels.values.astype('int32')\n",
    "\n",
    "testing_letters_images_scaled = testing_letters_images.values.astype('float32')/255\n",
    "testing_letters_labels = testing_letters_labels.values.astype('int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9PBNjIvGdQP",
    "outputId": "cc51c5bd-3fce-4ee3-9938-0d69cc4ada71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [ 1],\n",
       "       [ 2],\n",
       "       ...,\n",
       "       [27],\n",
       "       [28],\n",
       "       [28]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_letters_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FkJend9iFqYt",
    "outputId": "a13c3544-2554-4a71-8132-2fcf529893fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#categorical encoding \n",
    "#'Alef' --> '0 etc,\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# one hot encoding\n",
    "# number of classes = 28 (arabic alphabet classes)\n",
    "number_of_classes = 28\n",
    "\n",
    "training_letters_labels_encoded = to_categorical(training_letters_labels-1, num_classes=number_of_classes)\n",
    "testing_letters_labels_encoded = to_categorical(testing_letters_labels-1, num_classes=number_of_classes)\n",
    "print(training_letters_labels_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1upegzUGs_e",
    "outputId": "799bd2a8-ab33-446b-9355-81f6aa7fce21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13440, 32, 32, 1) (13440, 28) (3360, 32, 32, 1) (3360, 28)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " input shape should be (nb_samples,rows,columns,channels)\n",
    "\"\"\"\n",
    "# reshape input letter images to 32x32x1\n",
    "training_letters_images_scaled = training_letters_images_scaled.reshape([-1, 32, 32, 1])\n",
    "testing_letters_images_scaled = testing_letters_images_scaled.reshape([-1, 32, 32, 1])\n",
    "\n",
    "print(training_letters_images_scaled.shape, training_letters_labels_encoded.shape, testing_letters_images_scaled.shape, testing_letters_labels_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So_z_uymIB3S"
   },
   "source": [
    "**building The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTAn6k0zIGgL"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Dropout, Dense\n",
    "\n",
    "def create_model(optimizer='adam', kernel_initializer='he_normal', activation='relu'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=16, kernel_size=3, padding='same', input_shape=(32, 32, 1), kernel_initializer=kernel_initializer, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=32, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=3, padding='same', kernel_initializer=kernel_initializer, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    #Fully connected final layer\n",
    "    model.add(Dense(28, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1eIU3ReJcAK",
    "outputId": "7a04f741-d260-4aa4-c772-1a1260254255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 16)        160       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 16)       64        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 32)        4640      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16, 16, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8, 8, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 4, 4, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 28)                3612      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101,724\n",
      "Trainable params: 101,244\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rurhteOwJegf"
   },
   "outputs": [],
   "source": [
    "# import pydot\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# plot_model(model, to_file=\"model.png\", show_shapes=True)\n",
    "# from IPython.display import Image as IPythonImage\n",
    "# display(IPythonImage('model.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-vNZ_6TJuO6"
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# define the grid search parameters\n",
    "optimizer = ['RMSprop', 'Adam', 'Adagrad', 'Nadam']\n",
    "kernel_initializer = ['normal', 'uniform']\n",
    "activation = ['relu', 'linear', 'tanh']\n",
    "\n",
    "param_grid = dict(optimizer=optimizer, kernel_initializer=kernel_initializer, activation=activation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xRj4JNhIK1Ou",
    "outputId": "517a44ee-3dc2-4637-879c-8966a0d8ddf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13440, 32, 32, 1)\n",
      "(13440, 28)\n"
     ]
    }
   ],
   "source": [
    "print(training_letters_images_scaled.shape)\n",
    "print(training_letters_labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IC_TczrWKMei",
    "outputId": "54d4b165-2ae0-4333-9e81-205ec0879c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'RMSprop', 'kernel_initializer': 'normal', 'activation': 'relu'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 9s 9ms/step - loss: 1.4273 - accuracy: 0.5528 - val_loss: 1.7163 - val_accuracy: 0.4113\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.5308 - accuracy: 0.8211 - val_loss: 0.6634 - val_accuracy: 0.7821\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.3944 - accuracy: 0.8689 - val_loss: 0.3706 - val_accuracy: 0.8667\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.3336 - accuracy: 0.8906 - val_loss: 0.2453 - val_accuracy: 0.9226\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.2936 - accuracy: 0.9010 - val_loss: 0.3022 - val_accuracy: 0.8994\n",
      "=============================================================================\n",
      "{'optimizer': 'RMSprop', 'kernel_initializer': 'uniform', 'activation': 'relu'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 8s 9ms/step - loss: 1.2369 - accuracy: 0.6127 - val_loss: 0.6862 - val_accuracy: 0.7661\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 7s 10ms/step - loss: 0.4663 - accuracy: 0.8443 - val_loss: 0.2973 - val_accuracy: 0.8940\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 6s 10ms/step - loss: 0.3544 - accuracy: 0.8846 - val_loss: 0.5822 - val_accuracy: 0.8310\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.2981 - accuracy: 0.9042 - val_loss: 0.3850 - val_accuracy: 0.8836\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.2648 - accuracy: 0.9115 - val_loss: 0.2369 - val_accuracy: 0.9265\n",
      "=============================================================================\n",
      "{'optimizer': 'RMSprop', 'kernel_initializer': 'normal', 'activation': 'linear'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 8s 9ms/step - loss: 1.7475 - accuracy: 0.4611 - val_loss: 4.5607 - val_accuracy: 0.0964\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.7931 - accuracy: 0.7443 - val_loss: 0.6370 - val_accuracy: 0.7824\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 6s 10ms/step - loss: 0.5734 - accuracy: 0.8111 - val_loss: 1.6853 - val_accuracy: 0.5595\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.4610 - accuracy: 0.8440 - val_loss: 0.8755 - val_accuracy: 0.7312\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.4097 - accuracy: 0.8586 - val_loss: 0.6037 - val_accuracy: 0.8074\n",
      "=============================================================================\n",
      "{'optimizer': 'RMSprop', 'kernel_initializer': 'uniform', 'activation': 'linear'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 8s 9ms/step - loss: 1.5353 - accuracy: 0.5327 - val_loss: 4.2461 - val_accuracy: 0.1372\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 7s 10ms/step - loss: 0.6630 - accuracy: 0.7886 - val_loss: 2.9983 - val_accuracy: 0.3500\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.4896 - accuracy: 0.8360 - val_loss: 0.5531 - val_accuracy: 0.8205\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.4138 - accuracy: 0.8600 - val_loss: 0.7630 - val_accuracy: 0.7646\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.3628 - accuracy: 0.8814 - val_loss: 0.8585 - val_accuracy: 0.7458\n",
      "=============================================================================\n",
      "{'optimizer': 'RMSprop', 'kernel_initializer': 'normal', 'activation': 'tanh'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 8s 9ms/step - loss: 1.9208 - accuracy: 0.4107 - val_loss: 12.4915 - val_accuracy: 0.0393\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.8722 - accuracy: 0.7116 - val_loss: 6.3361 - val_accuracy: 0.1696\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.6488 - accuracy: 0.7808 - val_loss: 6.9076 - val_accuracy: 0.1140\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.5438 - accuracy: 0.8223 - val_loss: 2.0461 - val_accuracy: 0.4985\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.4856 - accuracy: 0.8373 - val_loss: 2.4408 - val_accuracy: 0.4286\n",
      "=============================================================================\n",
      "{'optimizer': 'RMSprop', 'kernel_initializer': 'uniform', 'activation': 'tanh'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 8s 10ms/step - loss: 1.5762 - accuracy: 0.5123 - val_loss: 7.6147 - val_accuracy: 0.1238\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.6470 - accuracy: 0.7843 - val_loss: 5.4840 - val_accuracy: 0.2440\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.5138 - accuracy: 0.8275 - val_loss: 5.6553 - val_accuracy: 0.1908\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.4443 - accuracy: 0.8499 - val_loss: 2.9901 - val_accuracy: 0.4440\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 6s 9ms/step - loss: 0.3964 - accuracy: 0.8673 - val_loss: 2.3018 - val_accuracy: 0.4926\n",
      "=============================================================================\n",
      "{'optimizer': 'Adam', 'kernel_initializer': 'normal', 'activation': 'relu'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 6s 8ms/step - loss: 1.5347 - accuracy: 0.5304 - val_loss: 1.7311 - val_accuracy: 0.4330\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.5945 - accuracy: 0.8067 - val_loss: 0.7538 - val_accuracy: 0.7619\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.4140 - accuracy: 0.8654 - val_loss: 0.5204 - val_accuracy: 0.8449\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.3282 - accuracy: 0.8938 - val_loss: 0.2664 - val_accuracy: 0.9199\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.3007 - accuracy: 0.8987 - val_loss: 0.2180 - val_accuracy: 0.9339\n",
      "=============================================================================\n",
      "{'optimizer': 'Adam', 'kernel_initializer': 'uniform', 'activation': 'relu'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 7s 8ms/step - loss: 1.4114 - accuracy: 0.5641 - val_loss: 0.6569 - val_accuracy: 0.7937\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.5490 - accuracy: 0.8193 - val_loss: 0.2877 - val_accuracy: 0.9128\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.3790 - accuracy: 0.8734 - val_loss: 1.2012 - val_accuracy: 0.6351\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.3139 - accuracy: 0.8978 - val_loss: 0.4054 - val_accuracy: 0.8557\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.2786 - accuracy: 0.9086 - val_loss: 0.2644 - val_accuracy: 0.9202\n",
      "=============================================================================\n",
      "{'optimizer': 'Adam', 'kernel_initializer': 'normal', 'activation': 'linear'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 6s 8ms/step - loss: 1.9134 - accuracy: 0.4305 - val_loss: 6.2817 - val_accuracy: 0.0839\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.8176 - accuracy: 0.7440 - val_loss: 0.5391 - val_accuracy: 0.8229\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.5873 - accuracy: 0.8138 - val_loss: 0.3467 - val_accuracy: 0.8902\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 5s 7ms/step - loss: 0.4755 - accuracy: 0.8449 - val_loss: 0.8198 - val_accuracy: 0.7321\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.4046 - accuracy: 0.8711 - val_loss: 0.6116 - val_accuracy: 0.7696\n",
      "=============================================================================\n",
      "{'optimizer': 'Adam', 'kernel_initializer': 'uniform', 'activation': 'linear'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 6s 8ms/step - loss: 1.6032 - accuracy: 0.5138 - val_loss: 4.6928 - val_accuracy: 0.1018\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.6892 - accuracy: 0.7827 - val_loss: 0.7900 - val_accuracy: 0.7280\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 5s 7ms/step - loss: 0.5132 - accuracy: 0.8395 - val_loss: 0.6512 - val_accuracy: 0.7973\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.4146 - accuracy: 0.8643 - val_loss: 0.3497 - val_accuracy: 0.8845\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.3664 - accuracy: 0.8804 - val_loss: 0.2236 - val_accuracy: 0.9339\n",
      "=============================================================================\n",
      "{'optimizer': 'Adam', 'kernel_initializer': 'normal', 'activation': 'tanh'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 6s 8ms/step - loss: 2.1232 - accuracy: 0.3618 - val_loss: 10.2889 - val_accuracy: 0.0372\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.9692 - accuracy: 0.6836 - val_loss: 5.9568 - val_accuracy: 0.1080\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.6763 - accuracy: 0.7762 - val_loss: 6.0090 - val_accuracy: 0.0893\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.5525 - accuracy: 0.8170 - val_loss: 4.1376 - val_accuracy: 0.1943\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 8s 12ms/step - loss: 0.4923 - accuracy: 0.8375 - val_loss: 0.9982 - val_accuracy: 0.6863\n",
      "=============================================================================\n",
      "{'optimizer': 'Adam', 'kernel_initializer': 'uniform', 'activation': 'tanh'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 7s 8ms/step - loss: 1.6628 - accuracy: 0.4945 - val_loss: 7.3794 - val_accuracy: 0.0512\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.6859 - accuracy: 0.7795 - val_loss: 7.5764 - val_accuracy: 0.0458\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.5086 - accuracy: 0.8307 - val_loss: 0.9383 - val_accuracy: 0.7211\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.4326 - accuracy: 0.8596 - val_loss: 4.5180 - val_accuracy: 0.1360\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 0.3798 - accuracy: 0.8747 - val_loss: 1.1394 - val_accuracy: 0.6622\n",
      "=============================================================================\n",
      "{'optimizer': 'Adagrad', 'kernel_initializer': 'normal', 'activation': 'relu'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 6s 8ms/step - loss: 3.3142 - accuracy: 0.1090 - val_loss: 2.8908 - val_accuracy: 0.1774\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.8020 - accuracy: 0.1921 - val_loss: 2.4117 - val_accuracy: 0.3458\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.5724 - accuracy: 0.2379 - val_loss: 2.2149 - val_accuracy: 0.3997\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.4086 - accuracy: 0.2802 - val_loss: 2.0595 - val_accuracy: 0.4435\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.2816 - accuracy: 0.3101 - val_loss: 1.9324 - val_accuracy: 0.4821\n",
      "=============================================================================\n",
      "{'optimizer': 'Adagrad', 'kernel_initializer': 'uniform', 'activation': 'relu'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 6s 8ms/step - loss: 2.9581 - accuracy: 0.1612 - val_loss: 2.8784 - val_accuracy: 0.1378\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.3883 - accuracy: 0.2860 - val_loss: 1.9321 - val_accuracy: 0.4625\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.0906 - accuracy: 0.3617 - val_loss: 1.6427 - val_accuracy: 0.5423\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 1.8622 - accuracy: 0.4241 - val_loss: 1.4386 - val_accuracy: 0.6065\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 1.7031 - accuracy: 0.4760 - val_loss: 1.2925 - val_accuracy: 0.6521\n",
      "=============================================================================\n",
      "{'optimizer': 'Adagrad', 'kernel_initializer': 'normal', 'activation': 'linear'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 6s 8ms/step - loss: 3.2749 - accuracy: 0.0798 - val_loss: 3.1394 - val_accuracy: 0.0902\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 3.0167 - accuracy: 0.1258 - val_loss: 2.8249 - val_accuracy: 0.1946\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.8866 - accuracy: 0.1597 - val_loss: 2.7220 - val_accuracy: 0.2226\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 6s 8ms/step - loss: 2.8086 - accuracy: 0.1829 - val_loss: 2.6345 - val_accuracy: 0.2399\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.7278 - accuracy: 0.2041 - val_loss: 2.5546 - val_accuracy: 0.2604\n",
      "=============================================================================\n",
      "{'optimizer': 'Adagrad', 'kernel_initializer': 'uniform', 'activation': 'linear'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 6s 8ms/step - loss: 3.0844 - accuracy: 0.1258 - val_loss: 3.1445 - val_accuracy: 0.0786\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.6778 - accuracy: 0.2211 - val_loss: 2.3954 - val_accuracy: 0.2961\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.4056 - accuracy: 0.2958 - val_loss: 2.1306 - val_accuracy: 0.3571\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.2250 - accuracy: 0.3400 - val_loss: 1.9552 - val_accuracy: 0.4268\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.0988 - accuracy: 0.3823 - val_loss: 1.8425 - val_accuracy: 0.4560\n",
      "=============================================================================\n",
      "{'optimizer': 'Adagrad', 'kernel_initializer': 'normal', 'activation': 'tanh'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 6s 8ms/step - loss: 3.4085 - accuracy: 0.0586 - val_loss: 3.0450 - val_accuracy: 0.1381\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 3.1232 - accuracy: 0.1103 - val_loss: 2.8467 - val_accuracy: 0.1943\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 3.0083 - accuracy: 0.1336 - val_loss: 2.7252 - val_accuracy: 0.2253\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.9139 - accuracy: 0.1600 - val_loss: 2.6287 - val_accuracy: 0.2485\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.8410 - accuracy: 0.1790 - val_loss: 2.5524 - val_accuracy: 0.2661\n",
      "=============================================================================\n",
      "{'optimizer': 'Adagrad', 'kernel_initializer': 'uniform', 'activation': 'tanh'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 6s 8ms/step - loss: 3.1210 - accuracy: 0.1160 - val_loss: 2.8916 - val_accuracy: 0.1619\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.6849 - accuracy: 0.2130 - val_loss: 2.2380 - val_accuracy: 0.3432\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.4159 - accuracy: 0.2799 - val_loss: 1.9846 - val_accuracy: 0.4000\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.2202 - accuracy: 0.3262 - val_loss: 1.8035 - val_accuracy: 0.4554\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 5s 8ms/step - loss: 2.0711 - accuracy: 0.3682 - val_loss: 1.6680 - val_accuracy: 0.4914\n",
      "=============================================================================\n",
      "{'optimizer': 'Nadam', 'kernel_initializer': 'normal', 'activation': 'relu'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 10s 12ms/step - loss: 1.5425 - accuracy: 0.5227 - val_loss: 1.7185 - val_accuracy: 0.4750\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 7s 11ms/step - loss: 0.5749 - accuracy: 0.8165 - val_loss: 0.3371 - val_accuracy: 0.8952\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 7s 11ms/step - loss: 0.4139 - accuracy: 0.8651 - val_loss: 0.5382 - val_accuracy: 0.7905\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 7s 11ms/step - loss: 0.3335 - accuracy: 0.8932 - val_loss: 0.2802 - val_accuracy: 0.9051\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 7s 11ms/step - loss: 0.2780 - accuracy: 0.9077 - val_loss: 0.2078 - val_accuracy: 0.9339\n",
      "=============================================================================\n",
      "{'optimizer': 'Nadam', 'kernel_initializer': 'uniform', 'activation': 'relu'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 10s 11ms/step - loss: 1.3439 - accuracy: 0.5969 - val_loss: 0.5838 - val_accuracy: 0.8271\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 7s 11ms/step - loss: 0.4805 - accuracy: 0.8481 - val_loss: 0.4866 - val_accuracy: 0.8473\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 8s 11ms/step - loss: 0.3748 - accuracy: 0.8774 - val_loss: 0.3969 - val_accuracy: 0.8717\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 7s 11ms/step - loss: 0.3037 - accuracy: 0.9012 - val_loss: 0.3217 - val_accuracy: 0.9036\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 8s 11ms/step - loss: 0.2629 - accuracy: 0.9137 - val_loss: 1.0709 - val_accuracy: 0.6461\n",
      "=============================================================================\n",
      "{'optimizer': 'Nadam', 'kernel_initializer': 'normal', 'activation': 'linear'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 10s 11ms/step - loss: 1.7784 - accuracy: 0.4675 - val_loss: 4.4096 - val_accuracy: 0.1399\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 8s 11ms/step - loss: 0.7922 - accuracy: 0.7516 - val_loss: 0.4900 - val_accuracy: 0.8435\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 8s 11ms/step - loss: 0.5621 - accuracy: 0.8181 - val_loss: 1.5091 - val_accuracy: 0.5699\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 7s 11ms/step - loss: 0.4601 - accuracy: 0.8493 - val_loss: 0.4863 - val_accuracy: 0.8310\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 7s 11ms/step - loss: 0.3939 - accuracy: 0.8719 - val_loss: 0.4495 - val_accuracy: 0.8443\n",
      "=============================================================================\n",
      "{'optimizer': 'Nadam', 'kernel_initializer': 'uniform', 'activation': 'linear'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 10s 11ms/step - loss: 1.6114 - accuracy: 0.5103 - val_loss: 1.2842 - val_accuracy: 0.5943\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 8s 11ms/step - loss: 0.6855 - accuracy: 0.7838 - val_loss: 1.0827 - val_accuracy: 0.6333\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 7s 11ms/step - loss: 0.5066 - accuracy: 0.8382 - val_loss: 0.6019 - val_accuracy: 0.7961\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 7s 11ms/step - loss: 0.4239 - accuracy: 0.8626 - val_loss: 0.9271 - val_accuracy: 0.7137\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 8s 11ms/step - loss: 0.3662 - accuracy: 0.8815 - val_loss: 0.2927 - val_accuracy: 0.8970\n",
      "=============================================================================\n",
      "{'optimizer': 'Nadam', 'kernel_initializer': 'normal', 'activation': 'tanh'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 10s 12ms/step - loss: 2.0963 - accuracy: 0.3718 - val_loss: 9.9613 - val_accuracy: 0.0518\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 8s 11ms/step - loss: 0.9029 - accuracy: 0.7097 - val_loss: 9.7235 - val_accuracy: 0.0670\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 8s 11ms/step - loss: 0.6511 - accuracy: 0.7872 - val_loss: 5.3624 - val_accuracy: 0.1574\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 7s 11ms/step - loss: 0.5382 - accuracy: 0.8203 - val_loss: 5.7979 - val_accuracy: 0.0943\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 7s 11ms/step - loss: 0.4754 - accuracy: 0.8399 - val_loss: 1.7592 - val_accuracy: 0.4911\n",
      "=============================================================================\n",
      "{'optimizer': 'Nadam', 'kernel_initializer': 'uniform', 'activation': 'tanh'}\n",
      "Epoch 1/5\n",
      "672/672 [==============================] - 10s 12ms/step - loss: 1.5930 - accuracy: 0.5173 - val_loss: 11.6117 - val_accuracy: 0.0363\n",
      "Epoch 2/5\n",
      "672/672 [==============================] - 8s 11ms/step - loss: 0.6645 - accuracy: 0.7795 - val_loss: 6.6894 - val_accuracy: 0.1604\n",
      "Epoch 3/5\n",
      "672/672 [==============================] - 8s 11ms/step - loss: 0.5049 - accuracy: 0.8377 - val_loss: 7.1702 - val_accuracy: 0.1339\n",
      "Epoch 4/5\n",
      "672/672 [==============================] - 8s 11ms/step - loss: 0.4240 - accuracy: 0.8609 - val_loss: 2.4762 - val_accuracy: 0.3878\n",
      "Epoch 5/5\n",
      "672/672 [==============================] - 8s 11ms/step - loss: 0.3888 - accuracy: 0.8733 - val_loss: 1.8308 - val_accuracy: 0.5735\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "# epochs = 5\n",
    "# batch_size = 20 # 20 divides the training data samples\n",
    "\n",
    "# #creating the models with different hyperparameters\n",
    "# for a,b,c in [(x,y,z) for x in optimizer for z in activation for y in kernel_initializer]:\n",
    "#     params = {'optimizer' : a , 'kernel_initializer' : b , 'activation' : c}\n",
    "#     print(params)\n",
    "#     curr_model = create_model(a, b, c)\n",
    "#     curr_model.fit(training_letters_images_scaled, training_letters_labels_encoded, \n",
    "#                     validation_data=(testing_letters_images_scaled, testing_letters_labels_encoded),\n",
    "#                     epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "#     print(\"=============================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9W084dFTQUbc"
   },
   "source": [
    "From the above results we can see that best parameters are:\n",
    "\n",
    "Optimizer: Adam\n",
    "\n",
    "Kernel_initializer: uniform\n",
    "\n",
    "Activation: relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-dfL5-6QRvC"
   },
   "outputs": [],
   "source": [
    "model = create_model(optimizer='Adam', kernel_initializer='uniform', activation='relu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mMoFEo1R4cu"
   },
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVJdCiehSC7K",
    "outputId": "e2c9180b-90e6-4b57-dd79-4439a8870be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "443/448 [============================>.] - ETA: 0s - loss: 1.4029 - accuracy: 0.5727\n",
      "Epoch 1: val_loss improved from inf to 6.53169, saving model to weights.hdf5\n",
      "448/448 [==============================] - 5s 9ms/step - loss: 1.3942 - accuracy: 0.5751 - val_loss: 6.5317 - val_accuracy: 0.0384\n",
      "Epoch 2/40\n",
      "443/448 [============================>.] - ETA: 0s - loss: 0.5251 - accuracy: 0.8318\n",
      "Epoch 2: val_loss improved from 6.53169 to 1.04839, saving model to weights.hdf5\n",
      "448/448 [==============================] - 4s 8ms/step - loss: 0.5234 - accuracy: 0.8321 - val_loss: 1.0484 - val_accuracy: 0.6595\n",
      "Epoch 3/40\n",
      "445/448 [============================>.] - ETA: 0s - loss: 0.3917 - accuracy: 0.8755\n",
      "Epoch 3: val_loss improved from 1.04839 to 0.42086, saving model to weights.hdf5\n",
      "448/448 [==============================] - 4s 8ms/step - loss: 0.3912 - accuracy: 0.8757 - val_loss: 0.4209 - val_accuracy: 0.8753\n",
      "Epoch 4/40\n",
      "445/448 [============================>.] - ETA: 0s - loss: 0.3219 - accuracy: 0.8966\n",
      "Epoch 4: val_loss improved from 0.42086 to 0.30852, saving model to weights.hdf5\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.3218 - accuracy: 0.8967 - val_loss: 0.3085 - val_accuracy: 0.9003\n",
      "Epoch 5/40\n",
      "447/448 [============================>.] - ETA: 0s - loss: 0.2926 - accuracy: 0.9048\n",
      "Epoch 5: val_loss did not improve from 0.30852\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.2927 - accuracy: 0.9048 - val_loss: 0.5839 - val_accuracy: 0.8190\n",
      "Epoch 6/40\n",
      "448/448 [==============================] - ETA: 0s - loss: 0.2534 - accuracy: 0.9158\n",
      "Epoch 6: val_loss improved from 0.30852 to 0.19847, saving model to weights.hdf5\n",
      "448/448 [==============================] - 4s 8ms/step - loss: 0.2534 - accuracy: 0.9158 - val_loss: 0.1985 - val_accuracy: 0.9357\n",
      "Epoch 7/40\n",
      "446/448 [============================>.] - ETA: 0s - loss: 0.2361 - accuracy: 0.9229\n",
      "Epoch 7: val_loss did not improve from 0.19847\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.2367 - accuracy: 0.9226 - val_loss: 0.9543 - val_accuracy: 0.6812\n",
      "Epoch 8/40\n",
      "445/448 [============================>.] - ETA: 0s - loss: 0.2253 - accuracy: 0.9246\n",
      "Epoch 8: val_loss did not improve from 0.19847\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.2250 - accuracy: 0.9247 - val_loss: 0.2338 - val_accuracy: 0.9226\n",
      "Epoch 9/40\n",
      "443/448 [============================>.] - ETA: 0s - loss: 0.2126 - accuracy: 0.9308\n",
      "Epoch 9: val_loss improved from 0.19847 to 0.13872, saving model to weights.hdf5\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.2121 - accuracy: 0.9310 - val_loss: 0.1387 - val_accuracy: 0.9577\n",
      "Epoch 10/40\n",
      "447/448 [============================>.] - ETA: 0s - loss: 0.1867 - accuracy: 0.9401\n",
      "Epoch 10: val_loss did not improve from 0.13872\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1866 - accuracy: 0.9401 - val_loss: 0.5495 - val_accuracy: 0.8396\n",
      "Epoch 11/40\n",
      "445/448 [============================>.] - ETA: 0s - loss: 0.1820 - accuracy: 0.9380\n",
      "Epoch 11: val_loss did not improve from 0.13872\n",
      "448/448 [==============================] - 4s 8ms/step - loss: 0.1825 - accuracy: 0.9379 - val_loss: 0.1426 - val_accuracy: 0.9551\n",
      "Epoch 12/40\n",
      "446/448 [============================>.] - ETA: 0s - loss: 0.1825 - accuracy: 0.9405\n",
      "Epoch 12: val_loss did not improve from 0.13872\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1825 - accuracy: 0.9406 - val_loss: 0.6154 - val_accuracy: 0.8271\n",
      "Epoch 13/40\n",
      "442/448 [============================>.] - ETA: 0s - loss: 0.1756 - accuracy: 0.9393\n",
      "Epoch 13: val_loss did not improve from 0.13872\n",
      "448/448 [==============================] - 4s 8ms/step - loss: 0.1754 - accuracy: 0.9394 - val_loss: 0.2231 - val_accuracy: 0.9304\n",
      "Epoch 14/40\n",
      "448/448 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9436\n",
      "Epoch 14: val_loss did not improve from 0.13872\n",
      "448/448 [==============================] - 4s 8ms/step - loss: 0.1633 - accuracy: 0.9436 - val_loss: 0.2066 - val_accuracy: 0.9372\n",
      "Epoch 15/40\n",
      "442/448 [============================>.] - ETA: 0s - loss: 0.1614 - accuracy: 0.9455\n",
      "Epoch 15: val_loss did not improve from 0.13872\n",
      "448/448 [==============================] - 4s 8ms/step - loss: 0.1607 - accuracy: 0.9458 - val_loss: 1.1504 - val_accuracy: 0.6220\n",
      "Epoch 16/40\n",
      "441/448 [============================>.] - ETA: 0s - loss: 0.1591 - accuracy: 0.9467\n",
      "Epoch 16: val_loss did not improve from 0.13872\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1589 - accuracy: 0.9467 - val_loss: 0.5297 - val_accuracy: 0.8321\n",
      "Epoch 17/40\n",
      "442/448 [============================>.] - ETA: 0s - loss: 0.1443 - accuracy: 0.9495\n",
      "Epoch 17: val_loss improved from 0.13872 to 0.12325, saving model to weights.hdf5\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1441 - accuracy: 0.9496 - val_loss: 0.1232 - val_accuracy: 0.9622\n",
      "Epoch 18/40\n",
      "448/448 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9554\n",
      "Epoch 18: val_loss did not improve from 0.12325\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1354 - accuracy: 0.9554 - val_loss: 0.9861 - val_accuracy: 0.6693\n",
      "Epoch 19/40\n",
      "442/448 [============================>.] - ETA: 0s - loss: 0.1342 - accuracy: 0.9545\n",
      "Epoch 19: val_loss did not improve from 0.12325\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1356 - accuracy: 0.9542 - val_loss: 0.2552 - val_accuracy: 0.9220\n",
      "Epoch 20/40\n",
      "442/448 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.9532\n",
      "Epoch 20: val_loss did not improve from 0.12325\n",
      "448/448 [==============================] - 4s 8ms/step - loss: 0.1400 - accuracy: 0.9532 - val_loss: 0.1390 - val_accuracy: 0.9607\n",
      "Epoch 21/40\n",
      "448/448 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9563\n",
      "Epoch 21: val_loss did not improve from 0.12325\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1267 - accuracy: 0.9563 - val_loss: 0.2729 - val_accuracy: 0.9185\n",
      "Epoch 22/40\n",
      "445/448 [============================>.] - ETA: 0s - loss: 0.1290 - accuracy: 0.9542\n",
      "Epoch 22: val_loss did not improve from 0.12325\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1287 - accuracy: 0.9545 - val_loss: 0.5875 - val_accuracy: 0.8292\n",
      "Epoch 23/40\n",
      "446/448 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9605\n",
      "Epoch 23: val_loss did not improve from 0.12325\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1204 - accuracy: 0.9606 - val_loss: 0.1519 - val_accuracy: 0.9548\n",
      "Epoch 24/40\n",
      "442/448 [============================>.] - ETA: 0s - loss: 0.1146 - accuracy: 0.9604\n",
      "Epoch 24: val_loss did not improve from 0.12325\n",
      "448/448 [==============================] - 4s 8ms/step - loss: 0.1144 - accuracy: 0.9604 - val_loss: 0.1295 - val_accuracy: 0.9628\n",
      "Epoch 25/40\n",
      "448/448 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9558\n",
      "Epoch 25: val_loss did not improve from 0.12325\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1279 - accuracy: 0.9558 - val_loss: 0.2544 - val_accuracy: 0.9152\n",
      "Epoch 26/40\n",
      "447/448 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9619\n",
      "Epoch 26: val_loss did not improve from 0.12325\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1123 - accuracy: 0.9619 - val_loss: 0.1428 - val_accuracy: 0.9580\n",
      "Epoch 27/40\n",
      "442/448 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 0.9608\n",
      "Epoch 27: val_loss did not improve from 0.12325\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1172 - accuracy: 0.9607 - val_loss: 0.1753 - val_accuracy: 0.9420\n",
      "Epoch 28/40\n",
      "447/448 [============================>.] - ETA: 0s - loss: 0.1109 - accuracy: 0.9616\n",
      "Epoch 28: val_loss improved from 0.12325 to 0.11773, saving model to weights.hdf5\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1107 - accuracy: 0.9617 - val_loss: 0.1177 - val_accuracy: 0.9673\n",
      "Epoch 29/40\n",
      "446/448 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9630\n",
      "Epoch 29: val_loss did not improve from 0.11773\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1043 - accuracy: 0.9630 - val_loss: 0.9330 - val_accuracy: 0.7128\n",
      "Epoch 30/40\n",
      "442/448 [============================>.] - ETA: 0s - loss: 0.1110 - accuracy: 0.9606\n",
      "Epoch 30: val_loss did not improve from 0.11773\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1112 - accuracy: 0.9606 - val_loss: 0.6200 - val_accuracy: 0.8039\n",
      "Epoch 31/40\n",
      "447/448 [============================>.] - ETA: 0s - loss: 0.1040 - accuracy: 0.9650\n",
      "Epoch 31: val_loss did not improve from 0.11773\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.1038 - accuracy: 0.9650 - val_loss: 0.2499 - val_accuracy: 0.9277\n",
      "Epoch 32/40\n",
      "443/448 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9668\n",
      "Epoch 32: val_loss improved from 0.11773 to 0.11672, saving model to weights.hdf5\n",
      "448/448 [==============================] - 4s 10ms/step - loss: 0.0986 - accuracy: 0.9669 - val_loss: 0.1167 - val_accuracy: 0.9688\n",
      "Epoch 33/40\n",
      "447/448 [============================>.] - ETA: 0s - loss: 0.0989 - accuracy: 0.9660\n",
      "Epoch 33: val_loss did not improve from 0.11672\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.0989 - accuracy: 0.9660 - val_loss: 0.5056 - val_accuracy: 0.8622\n",
      "Epoch 34/40\n",
      "445/448 [============================>.] - ETA: 0s - loss: 0.0969 - accuracy: 0.9655\n",
      "Epoch 34: val_loss did not improve from 0.11672\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.0969 - accuracy: 0.9655 - val_loss: 0.3851 - val_accuracy: 0.8890\n",
      "Epoch 35/40\n",
      "445/448 [============================>.] - ETA: 0s - loss: 0.0964 - accuracy: 0.9664\n",
      "Epoch 35: val_loss did not improve from 0.11672\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.0971 - accuracy: 0.9664 - val_loss: 0.2282 - val_accuracy: 0.9387\n",
      "Epoch 36/40\n",
      "444/448 [============================>.] - ETA: 0s - loss: 0.0941 - accuracy: 0.9676\n",
      "Epoch 36: val_loss did not improve from 0.11672\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.0937 - accuracy: 0.9676 - val_loss: 0.1292 - val_accuracy: 0.9655\n",
      "Epoch 37/40\n",
      "447/448 [============================>.] - ETA: 0s - loss: 0.0947 - accuracy: 0.9655\n",
      "Epoch 37: val_loss did not improve from 0.11672\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.0947 - accuracy: 0.9655 - val_loss: 0.1437 - val_accuracy: 0.9557\n",
      "Epoch 38/40\n",
      "443/448 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9654\n",
      "Epoch 38: val_loss did not improve from 0.11672\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.0981 - accuracy: 0.9654 - val_loss: 0.1197 - val_accuracy: 0.9676\n",
      "Epoch 39/40\n",
      "447/448 [============================>.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9647\n",
      "Epoch 39: val_loss did not improve from 0.11672\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.0990 - accuracy: 0.9647 - val_loss: 0.1447 - val_accuracy: 0.9616\n",
      "Epoch 40/40\n",
      "447/448 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9706\n",
      "Epoch 40: val_loss did not improve from 0.11672\n",
      "448/448 [==============================] - 4s 9ms/step - loss: 0.0881 - accuracy: 0.9707 - val_loss: 0.1488 - val_accuracy: 0.9601\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "# using checkpoints to save model weights to be used later instead of training again on the same epochs.\n",
    "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
    "history = model.fit(training_letters_images_scaled, training_letters_labels_encoded, \n",
    "                    validation_data=(testing_letters_images_scaled, testing_letters_labels_encoded),\n",
    "                    epochs=40, batch_size=30, verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkTobDaLSRos"
   },
   "outputs": [],
   "source": [
    "#Load the Model with the Best Validation Loss\n",
    "\n",
    "model.load_weights('weights.hdf5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_aoBtoqSkqs"
   },
   "source": [
    "**Test model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4FGy5RZSqLI",
    "outputId": "0b1d4215-91d1-403d-e67b-78310b83f07c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 1s 4ms/step - loss: 0.1167 - accuracy: 0.9688\n",
      "Test Accuracy: 0.96875\n",
      "Test Loss: 0.11672487109899521\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "metrics = model.evaluate(testing_letters_images_scaled, testing_letters_labels_encoded, verbose=1)\n",
    "print(\"Test Accuracy: {}\".format(metrics[1]))\n",
    "print(\"Test Loss: {}\".format(metrics[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk9v6gTfVQ1b"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDki76JlVPq0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def get_predicted_classes(model, data, labels=None):\n",
    "    image_predictions = model.predict(data)\n",
    "    predicted_classes = np.argmax(image_predictions, axis=1)\n",
    "    true_classes = np.argmax(labels, axis=1)\n",
    "    return predicted_classes, true_classes, image_predictions\n",
    "\n",
    "\n",
    "def get_classification_report(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aiGCWgobVwMA",
    "outputId": "c4838544-506f-434f-f604-6747a2f3b5fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       120\n",
      "           1       1.00      0.99      1.00       120\n",
      "           2       0.91      0.95      0.93       120\n",
      "           3       0.93      0.96      0.94       120\n",
      "           4       0.98      0.98      0.98       120\n",
      "           5       0.97      0.96      0.97       120\n",
      "           6       0.99      0.98      0.99       120\n",
      "           7       0.97      0.93      0.95       120\n",
      "           8       0.93      0.96      0.94       120\n",
      "           9       0.92      1.00      0.96       120\n",
      "          10       0.99      0.90      0.94       120\n",
      "          11       0.98      1.00      0.99       120\n",
      "          12       0.98      1.00      0.99       120\n",
      "          13       0.95      0.98      0.97       120\n",
      "          14       0.98      0.95      0.97       120\n",
      "          15       0.95      0.97      0.96       120\n",
      "          16       0.98      0.95      0.97       120\n",
      "          17       0.97      0.99      0.98       120\n",
      "          18       1.00      0.99      1.00       120\n",
      "          19       0.94      0.97      0.95       120\n",
      "          20       0.97      0.92      0.94       120\n",
      "          21       0.98      0.99      0.98       120\n",
      "          22       0.98      1.00      0.99       120\n",
      "          23       1.00      0.97      0.99       120\n",
      "          24       1.00      0.90      0.95       120\n",
      "          25       0.97      0.95      0.96       120\n",
      "          26       0.95      0.97      0.96       120\n",
      "          27       0.99      0.99      0.99       120\n",
      "\n",
      "    accuracy                           0.97      3360\n",
      "   macro avg       0.97      0.97      0.97      3360\n",
      "weighted avg       0.97      0.97      0.97      3360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred, y_true, image_predictions = get_predicted_classes(model, testing_letters_images_scaled, testing_letters_labels_encoded)\n",
    "get_classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "z5j-vrgwV2uX",
    "outputId": "a3043dd1-4223-44ff-be2e-906a1402f948"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "errors = (y_pred - y_true != 0)\n",
    "\n",
    "\n",
    "Y_pred_classes_errors = y_pred[errors]\n",
    "Y_pred_errors = image_predictions[errors]\n",
    "Y_true_errors = y_true[errors]\n",
    "X_val_errors = testing_letters_images_scaled[errors]\n",
    "\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            image_array = np.flip(img_errors[error], 0)\n",
    "            image_array = rotate(img_errors[error], -90)\n",
    "            ax[row,col].imshow((image_array).reshape((32,32)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of the wrong predicted letters\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Arabic Hand written recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
